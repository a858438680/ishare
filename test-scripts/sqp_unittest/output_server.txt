Decide Execution Pace Time: 24060ms
Holistic Optimization Time: 25633ms
Root of Q17
	Agg [] [avg_yearly -> l_extendedprice] [17]
		Join $"l_partkey" === $"p_partkey" inner [17]
			Join $"l_partkey" === $"agg_l_partkey" and  $"l_quantity" < $"avg_quantity" inner [17]
				Scan lineitem [17]
				Select Out: [avg_quantity, agg_l_partkey] Alias: [agg_l_partkey -> l_partkey] [17]
					Agg [l_partkey] [avg_quantity -> l_quantity] [17]
						Scan lineitem [17]
			Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
==========================
Root of Q8
	Agg [o_year] [mkt_share -> n2_name,volume] [8]
		Select Out: [o_year, n2_name, volume] Alias: [o_year -> o_orderdate; volume -> l_extendedprice,l_discount] [8]
			Join $"s_nationkey" === $"n2_nationkey" inner [8]
				Join $"n1_regionkey" === $"r_regionkey" inner [8]
					Join $"c_nationkey" === $"n1_nationkey" inner [8]
						Join $"l_suppkey" === $"s_suppkey" inner [8]
							Join $"l_partkey" === $"p_partkey" inner [8]
								Mat Join $"l_orderkey" === $"o_orderkey" inner [3, 8]
								Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
							Scan supplier [8]
						Select Out: [n1_nationkey, n1_regionkey] Alias: [n1_regionkey -> n_regionkey; n1_nationkey -> n_nationkey] [8]
							Scan nation [8]
					Mat Filter [r_name === "AMERICA"] [5, 2, 8] [8]
				Select Out: [n2_name, n2_nationkey] Alias: [n2_nationkey -> n_nationkey; n2_name -> n_name] [8]
					Scan nation [8]
==========================
Root of Q11
	Select Out: [product_value, ps_partkey] Alias: [] [11]
		Join $"product_value" > $"small_value" cross [11]
			Agg [ps_partkey] [product_value -> ps_supplycost,ps_availqty] [11]
				Mat Join $"s_suppkey" === $"ps_suppkey" inner [11]
			Agg [] [small_value -> ps_supplycost,ps_availqty] [11]
				Mat Join $"s_suppkey" === $"ps_suppkey" inner [11]
==========================
Root of Q20
	Select Out: [s_name, s_address] Alias: [] [20]
		Join $"s_nationkey" === $"n_nationkey" inner [20]
			Join $"s_suppkey" === $"ps_suppkey" left_semi [20]
				Scan supplier [20]
				Select Out: [ps_suppkey] Alias: [] [20]
					Join $"ps_partkey" === $"p_partkey" left_semi [20]
						Join $"ps_partkey" === $"agg_l_partkey" and  $"ps_suppkey" === $"agg_l_suppkey" and $"ps_availqty" > $"agg_l_sum" inner [20]
							Scan partsupp [20]
							Select Out: [agg_l_sum, agg_l_partkey, agg_l_suppkey] Alias: [agg_l_suppkey -> l_suppkey; agg_l_partkey -> l_partkey] [20]
								Agg [l_suppkey, l_partkey] [agg_l_sum -> l_quantity] [20]
									Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
						Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
			Mat Filter [n_name === "SAUDI ARABIA"] [20, 21, 11] [21]
==========================
Root of Q2
	Select Out: [s_name, s_acctbal, s_address, s_comment, n_name, p_mfgr, s_phone, p_partkey] Alias: [] [2]
		Join ($"p_partkey" === $"min_partkey") and  ($"ps_supplycost" === $"min_supplycost") inner [2]
			Join $"s_suppkey" === $"ps_suppkey" inner [2]
				Mat Join $"n_nationkey" === $"s_nationkey" inner [5, 2]
				Join $"ps_partkey" === $"p_partkey" inner [2]
					Scan partsupp [2]
					Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
			Select Out: [min_partkey, min_supplycost] Alias: [min_partkey -> ps_partkey] [2]
				Agg [ps_partkey] [min_supplycost -> ps_supplycost] [2]
					Join $"s_suppkey" === $"ps_suppkey" inner [2]
						Mat Join $"n_nationkey" === $"s_nationkey" inner [5, 2]
						Scan partsupp [2]
==========================
Root of Q5
	Agg [n_name] [revenue -> l_extendedprice,l_discount] [5]
		Join $"s_suppkey" === $"l_suppkey" and  $"s_nationkey" === $"c_nationkey" inner [5]
			Mat Join $"n_nationkey" === $"s_nationkey" inner [5, 2]
			Join $"o_custkey" === $"c_custkey" inner [5]
				Join $"l_orderkey" === $"o_orderkey" inner [5]
					Scan lineitem [5]
					Mat Filter [o_orderdate between ("1994-01-01", "1994-09-01")] [5, 4] [5]
				Scan customer [5]
==========================
Root of Q14
	Agg [] [promo_revenue -> l_extendedprice,p_type,l_discount] [14]
		Join $"l_partkey" === $"p_partkey" inner [14]
			Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
			Scan part [14]
==========================
Root of Q4
	Agg [o_orderpriority] [order_count -> lit(1L)] [4]
		Join $"o_orderkey" === $"l_orderkey" left_semi [4]
			Mat Filter [o_orderdate between ("1994-01-01", "1994-09-01")] [5, 4] [5]
			Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
==========================
Root of Q13
	Agg [c_count] [custdist -> lit(1L)] [13]
		Agg [c_custkey] [c_count -> o_orderkey] [13]
			Join $"c_custkey" === $"o_custkey" left_outer [13]
				Scan customer [13]
				Scan orders [13]
==========================
Root of Q22
	Agg [cntrycode] [numcust -> lit(1L); totalacctbal -> c_acctbal] [22]
		Select Out: [c_acctbal, cntrycode] Alias: [cntrycode -> c_phone] [22]
			Join $"c_acctbal" > $"avg_acctbal" cross [22]
				Join $"c_custkey" === $"o_custkey" left_anti [22]
					Mat Filter [c_acctbal > 0.0, substring($"c_phone", 1, 2) isin ("13", "31")] [22] [22]
					Scan orders [22]
				Agg [] [avg_acctbal -> c_acctbal] [22]
					Mat Filter [c_acctbal > 0.0, substring($"c_phone", 1, 2) isin ("13", "31")] [22] [22]
==========================
Root of Q16
	Agg [p_brand, p_size, p_type] [supplier_cnt -> ps_suppkey] [16]
		Select Out: [p_brand, p_size, p_type, ps_suppkey] Alias: [] [16]
			Join $"ps_suppkey" === $"s_suppkey" left_anti [16]
				Join $"p_partkey" === $"ps_partkey" inner [16]
					Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
					Scan partsupp [16]
				Select Out: [s_suppkey] Alias: [] [16]
					Filter [s_comment like ("%Customer%Complaints%")] [16] [16]
						Scan supplier [16]
==========================
Root of Q7
	Agg [l_year, supp_nation, cust_nation] [revenue -> l_extendedprice,l_discount] [7]
		Select Out: [l_year, l_extendedprice, supp_nation, cust_nation, l_discount] Alias: [l_year -> l_shipdate] [7]
			Filter [] [7] [7]
				Join $"c_nationkey" === $"n2_nationkey" inner [7]
					Join $"s_nationkey" === $"n1_nationkey" inner [7]
						Join $"l_suppkey" === $"s_suppkey" inner [7]
							Join $"l_orderkey" === $"o_orderkey" inner [7]
								Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
								Join $"c_custkey" === $"o_custkey" inner [7]
									Scan customer [7]
									Scan orders [7]
							Scan supplier [7]
						Select Out: [n1_nationkey, supp_nation] Alias: [supp_nation -> n_name; n1_nationkey -> n_nationkey] [7]
							Scan nation [7]
					Select Out: [cust_nation, n2_nationkey] Alias: [n2_nationkey -> n_nationkey; cust_nation -> n_name] [7]
						Scan nation [7]
==========================
Root of Q1
	Agg [l_linestatus, l_returnflag] [sum_base_price -> l_extendedprice,l_discount; avg_disc -> l_discount; sum_qty -> l_quantity; avg_qty -> l_quantity; sum_disc_price -> l_extendedprice,l_discount; count_order -> lit(1L); avg_price -> l_extendedprice; sum_charge -> l_extendedprice,l_tax,l_discount] [1]
		Filter [l_shipdate <= "1998-09-01"] [1] [1]
			Scan lineitem [1]
==========================
Root of Q10
	Agg [c_phone, c_address, n_name, c_name, c_comment, c_acctbal, c_custkey] [revenue -> l_extendedprice,l_discount] [10]
		Join $"l_orderkey" === $"o_orderkey" inner [10]
			Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
			Join $"c_nationkey" === $"n_nationkey" inner [10]
				Mat Join $"c_custkey" === $"o_custkey" inner [3, 10, 8]
				Scan nation [10]
==========================
Root of Q19
	Agg [] [revenue -> l_extendedprice,l_discount] [19]
		Join $"l_partkey" === $"p_partkey" and  ((($"p_brand" === "Brand#12") and
($"p_container" isin("SM CASE", "SM BOX", "SM PACK", "SM PKG")) and
($"l_quantity" >= 1 and $"l_quantity" <= 11) and
($"p_size" between(1, 5))
)
or (($"p_brand" === "Brand#23") and
($"p_container" isin("MED BAG", "MED BOX", "MED PKG", "MED PACK")) and
($"l_quantity" >= 10 and $"l_quantity" <= 20) and
($"p_size" between(1, 10))
)
or (($"p_brand" === "Brand#34") and
($"p_container" isin("LG CASE", "LG BOX", "LG PACK", "LG PKG")) and
($"l_quantity" >= 20 and $"l_quantity" <= 30) and
($"p_size" between(1, 15)))) inner [19]
			Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
			Scan part [19]
==========================
Root of Q18
	Agg [o_totalprice, c_name, o_orderkey, c_custkey, o_orderdate] [sum_quantity -> l_quantity] [18]
		Join $"o_orderkey" === $"agg_orderkey" left_semi [18]
			Join $"l_orderkey" === $"o_orderkey" inner [18]
				Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
				Join $"c_custkey" === $"o_custkey" inner [18]
					Scan customer [18]
					Scan orders [18]
			Select Out: [agg_orderkey] Alias: [agg_orderkey -> l_orderkey] [18]
				Filter [] [18] [18]
					Agg [l_orderkey] [sum_quantity -> l_quantity] [18]
						Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
==========================
Root of Q9
	Agg [nation, o_year] [sum_profit -> amount] [9]
		Select Out: [nation, o_year, amount] Alias: [o_year -> o_orderdate; amount -> ps_supplycost,l_quantity,l_extendedprice,l_discount; nation -> n_name] [9]
			Join $"s_nationkey" === $"n_nationkey" inner [9]
				Join $"l_suppkey" === $"s_suppkey" inner [9]
					Join $"l_partkey" === $"ps_partkey" and  $"l_suppkey" === $"ps_suppkey" inner [9]
						Join $"l_orderkey" === $"o_orderkey" inner [9]
							Join $"l_partkey" === $"p_partkey" inner [9]
								Scan lineitem [9]
								Mat Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
							Scan orders [9]
						Scan partsupp [9]
					Scan supplier [9]
				Scan nation [9]
==========================
Root of Q21
	Agg [s_name] [numwait -> lit(1L)] [21]
		Join ($"l_orderkey" === $"l3_orderkey") and  ($"l_suppkey" =!= $"l3_suppkey") left_anti [21]
			Join ($"l_orderkey" === $"l2_orderkey") and  ($"l_suppkey" =!= $"l2_suppkey") left_semi [21]
				Join $"s_nationkey" === $"n_nationkey" inner [21]
					Join $"l_suppkey" === $"s_suppkey" inner [21]
						Mat Join $"l_orderkey" === $"o_orderkey" inner [12, 21]
						Scan supplier [21]
					Mat Filter [n_name === "SAUDI ARABIA"] [20, 21, 11] [21]
				Select Out: [l2_suppkey, l2_orderkey] Alias: [l2_orderkey -> l_orderkey; l2_suppkey -> l_suppkey] [21]
					Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
			Select Out: [l3_orderkey, l3_suppkey] Alias: [l3_suppkey -> l_suppkey; l3_orderkey -> l_orderkey] [21]
				Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
==========================
Root of Q3
	Select Out: [revenue, o_orderdate, o_shippriority, l_orderkey] Alias: [] [3]
		Agg [o_orderdate, o_shippriority, l_orderkey] [revenue -> l_extendedprice,l_discount] [3]
			Mat Join $"l_orderkey" === $"o_orderkey" inner [3, 8]
==========================
Root of Q12
	Agg [l_shipmode] [high_line_count -> o_orderpriority; low_line_count -> o_orderpriority] [12]
		Mat Join $"l_orderkey" === $"o_orderkey" inner [12, 21]
==========================
Root of Q15
	Select Out: [s_name, total_revenue, s_suppkey, s_address, s_phone] Alias: [] [15]
		Join $"total_revenue" >= $"max_revenue" cross [15]
			Join $"s_suppkey" === $"supplier_no" inner [15]
				Scan supplier [15]
				Mat Select Out: [total_revenue, supplier_no] Alias: [supplier_no -> l_suppkey] [15]
			Agg [] [max_revenue -> total_revenue] [15]
				Mat Select Out: [total_revenue, supplier_no] Alias: [supplier_no -> l_suppkey] [15]
==========================
Root of Q6
	Agg [] [revenue -> l_extendedprice,l_discount] [6]
		Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
==========================
Filter [p_name like ("forest%")] [9, 16, 20, 17, 2, 8] [20]
	Filter [p_size === 15, p_type like ("%BRASS")] [9, 16, 2, 20, 17, 8] [2]
		Filter [p_type === "ECONOMY ANODIZED STEEL"] [9, 16, 20, 17, 2, 8] [8]
			Filter [p_name like ("%green%")] [9, 16, 20, 17, 2, 8] [9]
				Filter [p_brand =!= "Brand#45", p_size isin (49, 15, 9)] [9, 16, 20, 17, 2, 8] [16]
					Filter [p_brand === "Brand#23", p_container === "MED BOX"] [9, 16, 20, 17, 2, 8] [17]
						Scan part [9, 16, 20, 17, 2, 8]
==========================
Join $"l_orderkey" === $"o_orderkey" inner [3, 8]
	Filter [l_shipdate > "1998-03-15"] [3, 8] [3]
		Scan lineitem [3, 8]
	Mat Join $"c_custkey" === $"o_custkey" inner [3, 10, 8]
==========================
Filter [r_name === "AMERICA"] [5, 2, 8] [8]
	Filter [r_name === "EUROPE"] [5, 2, 8] [2]
		Filter [r_name === "ASIA"] [5, 2, 8] [5]
			Scan region [5, 2, 8]
==========================
Join $"s_suppkey" === $"ps_suppkey" inner [11]
	Join $"s_nationkey" === $"n_nationkey" inner [11]
		Scan supplier [11]
		Mat Filter [n_name === "SAUDI ARABIA"] [20, 21, 11] [21]
	Scan partsupp [11]
==========================
Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
	Filter [l_shipdate between ("1994-01-01", "1995-01-01"), l_discount between (0.05, 0.07), l_quantity < 24.0] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [6]
		Filter [l_shipdate between ("1995-01-01", "1995-06-31")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [7]
			Filter [l_returnflag === "R", l_shipdate between ("1994-02-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [10]
				Filter [l_shipmode === "MAIL", l_receiptdate > l_commitdate, l_shipdate < l_commitdate, l_receiptdate === "1994-01-01"] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [12]
					Filter [l_shipdate between ("1994-09-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [14]
						Filter [l_shipdate between ("1995-01-01", "1995-04-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [15]
							Filter [l_shipdate between ("1994-01-01", "1994-06-31")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [18]
								Filter [l_shipinstruct === "DELIVER IN PERSON", l_shipmode isin ("AIR", "AIR REG")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [19]
									Filter [l_shipdate between ("1994-01-01", "1994-06-31")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [20]
										Scan lineitem [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14]
==========================
Filter [n_name === "SAUDI ARABIA"] [20, 21, 11] [21]
	Filter [n_name === "GERMANY"] [20, 21, 11] [11]
		Filter [n_name === "CANADA"] [20, 21, 11] [20]
			Scan nation [20, 21, 11]
==========================
Join $"n_nationkey" === $"s_nationkey" inner [5, 2]
	Join $"r_regionkey" === $"n_regionkey" inner [5, 2]
		Mat Filter [r_name === "AMERICA"] [5, 2, 8] [8]
		Scan nation [5, 2]
	Scan supplier [5, 2]
==========================
Filter [o_orderdate between ("1994-01-01", "1994-09-01")] [5, 4] [5]
	Filter [o_orderdate between ("1993-07-01", "1993-10-01")] [5, 4] [4]
		Scan orders [5, 4]
==========================
Filter [c_acctbal > 0.0, substring($"c_phone", 1, 2) isin ("13", "31")] [22] [22]
	Scan customer [22]
==========================
Join $"c_custkey" === $"o_custkey" inner [3, 10, 8]
	Filter [c_mktsegment === "BUILDING"] [3, 10, 8] [3]
		Scan customer [3, 10, 8]
	Filter [o_orderdate between ("1994-05-01", "1994-10-01")] [3, 10, 8] [10]
		Filter [o_orderdate < "1992-03-15"] [3, 10, 8] [3]
			Filter [o_orderdate between ("1994-01-01", "1994-08-31")] [3, 10, 8] [8]
				Scan orders [3, 10, 8]
==========================
Join $"l_orderkey" === $"o_orderkey" inner [12, 21]
	Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]
	Filter [o_orderstatus === "F"] [12, 21] [21]
		Scan orders [12, 21]
==========================
Select Out: [total_revenue, supplier_no] Alias: [supplier_no -> l_suppkey] [15]
	Agg [l_suppkey] [total_revenue -> l_extendedprice,l_discount] [15]
		Mat Filter [l_receiptdate > l_commitdate, l_shipdate between ("1994-07-01", "1994-10-01")] [15, 12, 19, 20, 6, 21, 18, 10, 7, 4, 14] [21, 4]


import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q0_17 extends TPCHQuery {
    
private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleAvg = new DoubleAvg
val doubleSum = new DoubleSum

val result = ((DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema))
.join(DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema)
.groupBy($"l_partkey")
.agg(
(doubleAvg($"l_quantity") * 0.2).as("avg_quantity"))
.select($"l_partkey".as("agg_l_partkey"), $"avg_quantity"), $"l_partkey" === $"agg_l_partkey" and  $"l_quantity" < $"avg_quantity", "inner"))
.join(loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8), $"l_partkey" === $"p_partkey", "inner")
.agg(
(doubleSum($"l_extendedprice") / 7.0).as("avg_yearly"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q0_17].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q1_8 extends TPCHQuery {
    
private val Q23_3_8 = new StructType()
.add("o_orderdate", "date")
.add("c_nationkey", "long")
.add("l_extendedprice", "double")
.add("o_shippriority", "int")
.add("l_orderkey", "long")
.add("l_discount", "double")

private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")

private val Q25_5_2_8 = new StructType()
.add("r_regionkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val udaf_q8 = new UDAF_Q8

val result = (((((loadSharedTable(spark, "Q23_3_8", Q23_3_8))
.join(loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8), $"l_partkey" === $"p_partkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema), $"l_suppkey" === $"s_suppkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema)
.select($"n_regionkey".as("n1_regionkey"), $"n_nationkey".as("n1_nationkey")), $"c_nationkey" === $"n1_nationkey", "inner"))
.join(loadSharedTable(spark, "Q25_5_2_8", Q25_5_2_8), $"n1_regionkey" === $"r_regionkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema)
.select($"n_name".as("n2_name"), $"n_nationkey".as("n2_nationkey")), $"s_nationkey" === $"n2_nationkey", "inner")
.select(year($"o_orderdate").as("o_year"), ($"l_extendedprice" * ($"l_discount" - 1) * -1).as("volume"), $"n2_name")
.groupBy($"o_year")
.agg(
udaf_q8($"n2_name", $"volume").as("mkt_share"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q1_8].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q2_11 extends TPCHQuery {
    
private val Q26_11 = new StructType()
.add("ps_supplycost", "double")
.add("ps_partkey", "long")
.add("ps_availqty", "int")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleSum = new DoubleSum

val result = (loadSharedTable(spark, "Q26_11", Q26_11)
.groupBy($"ps_partkey")
.agg(
doubleSum($"ps_supplycost" * $"ps_availqty").as("product_value")))
.join(loadSharedTable(spark, "Q26_11", Q26_11)
.agg(
doubleSum($"ps_supplycost" * $"ps_availqty" * 0.0001/SF).as("small_value")), $"product_value" > $"small_value", "cross")
.select($"ps_partkey", $"product_value")
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q2_11].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q3_20 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")

private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")

private val Q27_20_21_11 = new StructType()
.add("n_nationkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleSum = new DoubleSum

val result = ((DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema))
.join(((DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema))
.join(loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.groupBy($"l_partkey", $"l_suppkey")
.agg(
(doubleSum($"l_quantity") * 0.5).as("agg_l_sum"))
.select($"l_partkey".as("agg_l_partkey"), $"l_suppkey".as("agg_l_suppkey"), $"agg_l_sum"), $"ps_partkey" === $"agg_l_partkey" and  $"ps_suppkey" === $"agg_l_suppkey" and $"ps_availqty" > $"agg_l_sum", "inner"))
.join(loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8), $"ps_partkey" === $"p_partkey", "left_semi")
.select($"ps_suppkey"), $"s_suppkey" === $"ps_suppkey", "left_semi"))
.join(loadSharedTable(spark, "Q27_20_21_11", Q27_20_21_11), $"s_nationkey" === $"n_nationkey", "inner")
.select($"s_name", $"s_address")
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q3_20].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q4_2 extends TPCHQuery {
    
private val Q29_5_2 = new StructType()
.add("n_name", "string")
.add("s_acctbal", "double")
.add("s_suppkey", "long")
.add("s_nationkey", "long")
.add("s_address", "string")
.add("s_phone", "string")
.add("s_comment", "string")
.add("s_name", "string")

private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = ((loadSharedTable(spark, "Q29_5_2", Q29_5_2))
.join((DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema))
.join(loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8), $"ps_partkey" === $"p_partkey", "inner"), $"s_suppkey" === $"ps_suppkey", "inner"))
.join((loadSharedTable(spark, "Q29_5_2", Q29_5_2))
.join(DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema), $"s_suppkey" === $"ps_suppkey", "inner")
.groupBy($"ps_partkey")
.agg(
min($"ps_supplycost").as("min_supplycost"))
.select($"ps_partkey".as("min_partkey"), $"min_supplycost"), ($"p_partkey" === $"min_partkey") and  ($"ps_supplycost" === $"min_supplycost"), "inner")
.select($"s_acctbal", $"s_name", $"n_name", $"p_partkey", $"p_mfgr", $"s_address", $"s_phone", $"s_comment")
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q4_2].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q5_5 extends TPCHQuery {
    
private val Q29_5_2 = new StructType()
.add("n_name", "string")
.add("s_acctbal", "double")
.add("s_suppkey", "long")
.add("s_nationkey", "long")
.add("s_address", "string")
.add("s_phone", "string")
.add("s_comment", "string")
.add("s_name", "string")

private val Q30_5_4 = new StructType()
.add("o_orderkey", "long")
.add("o_orderpriority", "string")
.add("o_custkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_disc_price = new Sum_disc_price

val result = (loadSharedTable(spark, "Q29_5_2", Q29_5_2))
.join(((DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema))
.join(loadSharedTable(spark, "Q30_5_4", Q30_5_4), $"l_orderkey" === $"o_orderkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema), $"o_custkey" === $"c_custkey", "inner"), $"s_suppkey" === $"l_suppkey" and  $"s_nationkey" === $"c_nationkey", "inner")
.groupBy($"n_name")
.agg(
sum_disc_price($"l_extendedprice", $"l_discount").as("revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q5_5].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q6_14 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val udaf_q14 = new UDAF_Q14
val sum_disc_price = new Sum_disc_price

val result = (loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join(DataUtils.loadStreamTable(spark, "part", "p", tpchSchema), $"l_partkey" === $"p_partkey", "inner")
.agg(
((udaf_q14($"p_type", $"l_extendedprice", $"l_discount")/sum_disc_price($"l_extendedprice", $"l_discount")) * 100).as("promo_revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q6_14].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q7_4 extends TPCHQuery {
    
private val Q30_5_4 = new StructType()
.add("o_orderkey", "long")
.add("o_orderpriority", "string")
.add("o_custkey", "long")

private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val order_count = new Count

val result = (loadSharedTable(spark, "Q30_5_4", Q30_5_4))
.join(loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14), $"o_orderkey" === $"l_orderkey", "left_semi")
.groupBy($"o_orderpriority")
.agg(
order_count(lit(1L)).as("order_count"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q7_4].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q8_13 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val custdist = new Count
val count_not_null = new Count_not_null

val result = (DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema), $"c_custkey" === $"o_custkey", "left_outer")
.groupBy($"c_custkey")
.agg(
count_not_null($"o_orderkey").as("c_count"))
.groupBy($"c_count")
.agg(
custdist(lit(1L)).as("custdist"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q8_13].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q9_22 extends TPCHQuery {
    
private val Q31_22 = new StructType()
.add("c_custkey", "long")
.add("c_phone", "string")
.add("c_acctbal", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleAvg = new DoubleAvg
val doubleSum = new DoubleSum
val numcust = new Count

val result = ((loadSharedTable(spark, "Q31_22", Q31_22))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema), $"c_custkey" === $"o_custkey", "left_anti"))
.join(loadSharedTable(spark, "Q31_22", Q31_22)
.agg(
doubleAvg($"c_acctbal").as("avg_acctbal")), $"c_acctbal" > $"avg_acctbal", "cross")
.select(substring($"c_phone", 1, 2).as("cntrycode"), $"c_acctbal")
.groupBy($"cntrycode")
.agg(
numcust(lit(1L)).as("numcust"),
doubleSum($"c_acctbal").as("totalacctbal"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q9_22].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q10_16 extends TPCHQuery {
    
private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val supplier_cnt = new Count

val result = ((loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8))
.join(DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema), $"p_partkey" === $"ps_partkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema)
.filter($"s_comment" like ("%Customer%Complaints%"))
.select($"s_suppkey"), $"ps_suppkey" === $"s_suppkey", "left_anti")
.select($"p_brand", $"p_type", $"p_size", $"ps_suppkey")
.groupBy($"p_brand", $"p_type", $"p_size")
.agg(
supplier_cnt($"ps_suppkey").as("supplier_cnt"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q10_16].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q11_7 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_disc_price = new Sum_disc_price

val result = ((((loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join((DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema), $"c_custkey" === $"o_custkey", "inner"), $"l_orderkey" === $"o_orderkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema), $"l_suppkey" === $"s_suppkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema)
.select($"n_name".as("supp_nation"), $"n_nationkey".as("n1_nationkey")), $"s_nationkey" === $"n1_nationkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema)
.select($"n_name".as("cust_nation"), $"n_nationkey".as("n2_nationkey")), $"c_nationkey" === $"n2_nationkey", "inner")
.filter(($"supp_nation" === "FRANCE" and $"cust_nation" === "GERMANY") or ($"supp_nation" === "GERMANY" and $"cust_nation" === "FRANCE"))
.select($"supp_nation", $"cust_nation", year($"l_shipdate").as("l_year"), $"l_extendedprice", $"l_discount")
.groupBy($"supp_nation", $"cust_nation", $"l_year")
.agg(
sum_disc_price($"l_extendedprice", $"l_discount").as("revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q11_7].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q12_1 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_base_price = new DoubleSum
val avg_disc = new DoubleAvg
val sum_qty = new DoubleSum
val sum_disc_price = new Sum_disc_price
val avg_qty = new DoubleAvg
val count_order = new Count
val avg_price = new DoubleAvg
val sum_charge = new Sum_disc_price_with_tax

val result = DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema)
.filter($"l_shipdate" <= "1998-09-01")
.groupBy($"l_returnflag", $"l_linestatus")
.agg(
sum_qty($"l_quantity").as("sum_qty"),
sum_base_price($"l_extendedprice" * $"l_discount").as("sum_base_price"),
sum_disc_price($"l_extendedprice", $"l_discount").as("sum_disc_price"),
sum_charge($"l_extendedprice", $"l_discount", $"l_tax").as("sum_charge"),
avg_qty($"l_quantity").as("avg_qty"),
avg_price($"l_extendedprice").as("avg_price"),
avg_disc($"l_discount").as("avg_disc"),
count_order(lit(1L)).as("count_order"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q12_1].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q13_10 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")

private val Q24_3_10_8 = new StructType()
.add("c_custkey", "long")
.add("o_orderdate", "date")
.add("c_address", "string")
.add("c_nationkey", "long")
.add("c_name", "string")
.add("c_comment", "string")
.add("o_shippriority", "int")
.add("c_phone", "string")
.add("c_acctbal", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val revenue = new Sum_disc_price

val result = (loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join((loadSharedTable(spark, "Q24_3_10_8", Q24_3_10_8))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema), $"c_nationkey" === $"n_nationkey", "inner"), $"l_orderkey" === $"o_orderkey", "inner")
.groupBy($"c_custkey", $"c_name", $"c_acctbal", $"c_phone", $"n_name", $"c_address", $"c_comment")
.agg(
revenue($"l_extendedprice", $"l_discount").as("revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q13_10].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q14_19 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_disc_price = new Sum_disc_price

val result = (loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join(DataUtils.loadStreamTable(spark, "part", "p", tpchSchema), $"l_partkey" === $"p_partkey" and  ((($"p_brand" === "Brand#12") and
($"p_container" isin("SM CASE", "SM BOX", "SM PACK", "SM PKG")) and
($"l_quantity" >= 1 and $"l_quantity" <= 11) and
($"p_size" between(1, 5))
)
or (($"p_brand" === "Brand#23") and
($"p_container" isin("MED BAG", "MED BOX", "MED PKG", "MED PACK")) and
($"l_quantity" >= 10 and $"l_quantity" <= 20) and
($"p_size" between(1, 10))
)
or (($"p_brand" === "Brand#34") and
($"p_container" isin("LG CASE", "LG BOX", "LG PACK", "LG PKG")) and
($"l_quantity" >= 20 and $"l_quantity" <= 30) and
($"p_size" between(1, 15)))), "inner")
.agg(
sum_disc_price($"l_extendedprice", $"l_discount").as("revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q14_19].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q15_18 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleSum2 = new DoubleSum
val doubleSum1 = new DoubleSum

val result = ((loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join((DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema), $"c_custkey" === $"o_custkey", "inner"), $"l_orderkey" === $"o_orderkey", "inner"))
.join(loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.groupBy($"l_orderkey")
.agg(
doubleSum1($"l_quantity").as("sum_quantity"))
.filter($"sum_quantity" > 300)
.select($"l_orderkey".as("agg_orderkey")), $"o_orderkey" === $"agg_orderkey", "left_semi")
.groupBy($"c_name", $"c_custkey", $"o_orderkey", $"o_orderdate", $"o_totalprice")
.agg(
doubleSum2($"l_quantity").as("sum_quantity"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q15_18].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q16_9 extends TPCHQuery {
    
private val Q22_9_16_20_17_2_8 = new StructType()
.add("p_partkey", "long")
.add("p_size", "int")
.add("p_mfgr", "string")
.add("p_type", "string")
.add("p_brand", "string")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleSum = new DoubleSum

val result = (((((DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema))
.join(loadSharedTable(spark, "Q22_9_16_20_17_2_8", Q22_9_16_20_17_2_8), $"l_partkey" === $"p_partkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema), $"l_orderkey" === $"o_orderkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema), $"l_partkey" === $"ps_partkey" and  $"l_suppkey" === $"ps_suppkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema), $"l_suppkey" === $"s_suppkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema), $"s_nationkey" === $"n_nationkey", "inner")
.select($"n_name".as("nation"), year($"o_orderdate").as("o_year"), (($"l_extendedprice" * ($"l_discount" - 1) * -1) - $"ps_supplycost" * $"l_quantity").as("amount"))
.groupBy($"nation", $"o_year")
.agg(
doubleSum($"amount").as("sum_profit"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q16_9].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q17_21 extends TPCHQuery {
    
private val Q32_12_21 = new StructType()
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("o_orderpriority", "string")
.add("l_orderkey", "long")

private val Q27_20_21_11 = new StructType()
.add("n_nationkey", "long")

private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val count = new Count

val result = ((((loadSharedTable(spark, "Q32_12_21", Q32_12_21))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema), $"l_suppkey" === $"s_suppkey", "inner"))
.join(loadSharedTable(spark, "Q27_20_21_11", Q27_20_21_11), $"s_nationkey" === $"n_nationkey", "inner"))
.join(loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.select($"l_orderkey".as("l2_orderkey"), $"l_suppkey".as("l2_suppkey")), ($"l_orderkey" === $"l2_orderkey") and  ($"l_suppkey" =!= $"l2_suppkey"), "left_semi"))
.join(loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.select($"l_orderkey".as("l3_orderkey"), $"l_suppkey".as("l3_suppkey")), ($"l_orderkey" === $"l3_orderkey") and  ($"l_suppkey" =!= $"l3_suppkey"), "left_anti")
.groupBy($"s_name")
.agg(
count(lit(1L)).as("numwait"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q17_21].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q18_3 extends TPCHQuery {
    
private val Q23_3_8 = new StructType()
.add("o_orderdate", "date")
.add("c_nationkey", "long")
.add("l_extendedprice", "double")
.add("o_shippriority", "int")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_disc_price = new Sum_disc_price

val result = loadSharedTable(spark, "Q23_3_8", Q23_3_8)
.groupBy($"l_orderkey", $"o_orderdate", $"o_shippriority")
.agg(
sum_disc_price($"l_extendedprice", $"l_discount").as("revenue"))
.select($"l_orderkey", $"revenue", $"o_orderdate", $"o_shippriority")
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q18_3].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q19_12 extends TPCHQuery {
    
private val Q32_12_21 = new StructType()
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("o_orderpriority", "string")
.add("l_orderkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val udaf_q12_low = new UDAF_Q12_LOW
val udaf_q12_high = new UDAF_Q12_HIGH

val result = loadSharedTable(spark, "Q32_12_21", Q32_12_21)
.groupBy($"l_shipmode")
.agg(
udaf_q12_high($"o_orderpriority").as("high_line_count"),
udaf_q12_low($"o_orderpriority").as("low_line_count"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q19_12].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q20_15 extends TPCHQuery {
    
private val Q33_15 = new StructType()
.add("total_revenue", "double")
.add("supplier_no", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = ((DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema))
.join(loadSharedTable(spark, "Q33_15", Q33_15), $"s_suppkey" === $"supplier_no", "inner"))
.join(loadSharedTable(spark, "Q33_15", Q33_15)
.agg(
max($"total_revenue").as("max_revenue")), $"total_revenue" >= $"max_revenue", "cross")
.select($"s_suppkey", $"s_name", $"s_address", $"s_phone", $"total_revenue")
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q20_15].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q21_6 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val doubleSum = new DoubleSum

val result = loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.agg(
doubleSum($"l_extendedprice" * $"l_discount").as("revenue"))
.select("*")
 DataUtils.writeToSinkWithExtraOptions(
   result, query_name, uid, numBatch, constraint)
        
    }
}

scala.reflect.classTag[Q21_6].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q22_9_16_20_17_2_8 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "part", "p", tpchSchema)
.filter($"p_brand" === "Brand#23" and $"p_container" === "MED BOX")
.filter(($"p_brand" =!= "Brand#45") and ($"p_size" isin (49, 15, 9)))
.filter($"p_name" like ("%green%"))
.filter($"p_type" === "ECONOMY ANODIZED STEEL")
.filter(($"p_size" === 15) and ($"p_type" like ("%BRASS")))
.filter($"p_name" like ("forest%"))
.select($"p_partkey", $"p_size", $"p_mfgr", $"p_type", $"p_brand")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q22_9_16_20_17_2_8", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q22_9_16_20_17_2_8].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q23_3_8 extends TPCHQuery {
    
private val Q24_3_10_8 = new StructType()
.add("c_custkey", "long")
.add("o_orderdate", "date")
.add("c_address", "string")
.add("c_nationkey", "long")
.add("c_name", "string")
.add("c_comment", "string")
.add("o_shippriority", "int")
.add("c_phone", "string")
.add("c_acctbal", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = (DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema)
.filter($"l_shipdate" > "1998-03-15"))
.join(loadSharedTable(spark, "Q24_3_10_8", Q24_3_10_8), $"l_orderkey" === $"o_orderkey", "inner")
.select($"o_orderdate", $"c_nationkey", $"l_extendedprice", $"o_shippriority", $"l_orderkey", $"l_discount")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q23_3_8", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q23_3_8].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q24_3_10_8 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = (DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema)
.filter($"c_mktsegment" === "BUILDING"))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema)
.filter($"o_orderdate" between ("1994-01-01", "1994-08-31"))
.filter($"o_orderdate" < "1992-03-15")
.filter($"o_orderdate" between ("1994-05-01", "1994-10-01")), $"c_custkey" === $"o_custkey", "inner")
.select($"c_custkey", $"o_orderdate", $"c_address", $"c_nationkey", $"c_name", $"c_comment", $"o_shippriority", $"c_phone", $"c_acctbal")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q24_3_10_8", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q24_3_10_8].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q25_5_2_8 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "region", "r", tpchSchema)
.filter($"r_name" === "ASIA")
.filter($"r_name" === "EUROPE")
.filter($"r_name" === "AMERICA")
.select($"r_regionkey")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q25_5_2_8", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q25_5_2_8].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q26_11 extends TPCHQuery {
    
private val Q27_20_21_11 = new StructType()
.add("n_nationkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = ((DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema))
.join(loadSharedTable(spark, "Q27_20_21_11", Q27_20_21_11), $"s_nationkey" === $"n_nationkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "partsupp", "ps", tpchSchema), $"s_suppkey" === $"ps_suppkey", "inner")
.select($"ps_supplycost", $"ps_partkey", $"ps_availqty")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q26_11", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q26_11].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q27_20_21_11 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema)
.filter($"n_name" === "CANADA")
.filter($"n_name" === "GERMANY")
.filter($"n_name" === "SAUDI ARABIA")
.select($"n_nationkey")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q27_20_21_11", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q27_20_21_11].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q28_15_12_19_20_6_21_18_10_7_4_14 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "lineitem", "l", tpchSchema)
.filter($"l_shipdate" between ("1994-01-01", "1994-06-31"))
.filter(($"l_shipinstruct" === "DELIVER IN PERSON") and ($"l_shipmode" isin ("AIR", "AIR REG")))
.filter($"l_shipdate" between ("1994-01-01", "1994-06-31"))
.filter($"l_shipdate" between ("1995-01-01", "1995-04-01"))
.filter($"l_shipdate" between ("1994-09-01", "1994-10-01"))
.filter(($"l_shipmode" === "MAIL") and ($"l_receiptdate" > $"l_commitdate") and ($"l_shipdate" < $"l_commitdate") and ($"l_receiptdate" === "1994-01-01"))
.filter(($"l_returnflag" === "R") and ($"l_shipdate" between ("1994-02-01", "1994-10-01")))
.filter($"l_shipdate" between ("1995-01-01", "1995-06-31"))
.filter(($"l_shipdate" between ("1994-01-01", "1995-01-01")) and ($"l_discount" between (0.05, 0.07)) and ($"l_quantity" < 24.0))
.filter(($"l_receiptdate" > $"l_commitdate") and ($"l_shipdate" between ("1994-07-01", "1994-10-01")))
.select($"l_shipdate", $"l_partkey", $"l_extendedprice", $"l_quantity", $"l_shipmode", $"l_suppkey", $"l_orderkey", $"l_discount")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q28_15_12_19_20_6_21_18_10_7_4_14", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q28_15_12_19_20_6_21_18_10_7_4_14].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q29_5_2 extends TPCHQuery {
    
private val Q25_5_2_8 = new StructType()
.add("r_regionkey", "long")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = ((loadSharedTable(spark, "Q25_5_2_8", Q25_5_2_8))
.join(DataUtils.loadStreamTable(spark, "nation", "n", tpchSchema), $"r_regionkey" === $"n_regionkey", "inner"))
.join(DataUtils.loadStreamTable(spark, "supplier", "s", tpchSchema), $"n_nationkey" === $"s_nationkey", "inner")
.select($"n_name", $"s_acctbal", $"s_suppkey", $"s_nationkey", $"s_address", $"s_phone", $"s_comment", $"s_name")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q29_5_2", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q29_5_2].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q30_5_4 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema)
.filter($"o_orderdate" between ("1993-07-01", "1993-10-01"))
.filter($"o_orderdate" between ("1994-01-01", "1994-09-01"))
.select($"o_orderkey", $"o_orderpriority", $"o_custkey")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q30_5_4", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q30_5_4].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q31_22 extends TPCHQuery {
    


    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = DataUtils.loadStreamTable(spark, "customer", "c", tpchSchema)
.filter(($"c_acctbal" > 0.0) and (substring($"c_phone", 1, 2) isin ("13", "31")))
.select($"c_custkey", $"c_phone", $"c_acctbal")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q31_22", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q31_22].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q32_12_21 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      

val result = (loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14))
.join(DataUtils.loadStreamTable(spark, "orders", "o", tpchSchema)
.filter($"o_orderstatus" === "F"), $"l_orderkey" === $"o_orderkey", "inner")
.select($"l_shipmode", $"l_suppkey", $"o_orderpriority", $"l_orderkey")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q32_12_21", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q32_12_21].runtimeClass

      



import totem.middleground.tpch._
import totem.middleground.sqp.tpchquery.TPCHQuery

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.avro.{from_avro, SchemaConverters}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

private class Q33_15 extends TPCHQuery {
    
private val Q28_15_12_19_20_6_21_18_10_7_4_14 = new StructType()
.add("l_shipdate", "date")
.add("l_partkey", "long")
.add("l_extendedprice", "double")
.add("l_quantity", "double")
.add("l_shipmode", "string")
.add("l_suppkey", "long")
.add("l_orderkey", "long")
.add("l_discount", "double")



    override def execQuery(spark: SparkSession, tpchSchema: TPCHSchema): Unit = {
       
import spark.implicits._
      
val sum_disc_price = new Sum_disc_price

val result = loadSharedTable(spark, "Q28_15_12_19_20_6_21_18_10_7_4_14", Q28_15_12_19_20_6_21_18_10_7_4_14)
.groupBy($"l_suppkey")
.agg(
sum_disc_price($"l_extendedprice", $"l_discount").as("total_revenue"))
.select($"l_suppkey".as("supplier_no"), $"total_revenue")
.select($"total_revenue", $"supplier_no")
 DataUtils.writeToKafkaWithExtraOptions(
    result, "Q33_15", query_name, uid,
       numBatch, constraint, tpchSchema.checkpointLocation)
         
    }
}

scala.reflect.classTag[Q33_15].runtimeClass

      

Query Config

Q22_9_16_20_17_2_8
8 -> [p_type = ECONOMY ANODIZED STEEL]
17 -> [p_brand = Brand#23, p_container = MED BOX]
20 -> [p_name StartsWith forest]
2 -> [p_size = 15, p_type EndsWith BRASS]
16 -> [p_brand != Brand#45]
9 -> [p_name Contains green]

Q23_3_8
3 -> [l_shipdate > 1998-03-15]

Q24_3_10_8
8 -> [o_orderdate <= 1994-08-31, o_orderdate >= 1994-01-01]
10 -> [o_orderdate <= 1994-10-01, o_orderdate >= 1994-05-01]
3 -> [c_mktsegment = BUILDING, o_orderdate < 1992-03-15]

Q25_5_2_8
8 -> [r_name = AMERICA]
2 -> [r_name = EUROPE]
5 -> [r_name = ASIA]

Q27_20_21_11
11 -> [n_name = GERMANY]
20 -> [n_name = CANADA]
21 -> [n_name = SAUDI ARABIA]

Q28_15_12_19_20_6_21_18_10_7_4_14
20 -> [l_shipdate <= 1994-06-31, l_shipdate >= 1994-01-01]
14 -> [l_shipdate <= 1994-10-01, l_shipdate >= 1994-09-01]
4 -> [l_shipdate >= 1994-07-01, l_shipdate <= 1994-10-01, l_receiptdate > l_commitdate]
7 -> [l_shipdate >= 1995-01-01, l_shipdate <= 1995-06-31]
10 -> [l_shipdate <= 1994-10-01, l_returnflag = R, l_shipdate >= 1994-02-01]
19 -> [l_shipinstruct = DELIVER IN PERSON]
18 -> [l_shipdate <= 1994-06-31, l_shipdate >= 1994-01-01]
21 -> [l_shipdate >= 1994-07-01, l_shipdate <= 1994-10-01, l_receiptdate > l_commitdate]
12 -> [l_receiptdate = 1994-01-01, l_receiptdate > l_commitdate, l_shipmode = MAIL, l_shipdate < l_commitdate]
6 -> [l_discount <= 0.07, l_quantity < 24.0, l_discount >= 0.05, l_shipdate >= 1994-01-01, l_shipdate <= 1995-01-01]
15 -> [l_shipdate <= 1995-04-01, l_shipdate >= 1995-01-01]

Q30_5_4
5 -> [o_orderdate >= 1994-01-01, o_orderdate <= 1994-09-01]
4 -> [o_orderdate <= 1993-10-01, o_orderdate >= 1993-07-01]

Q32_12_21
21 -> [o_orderstatus = F]

